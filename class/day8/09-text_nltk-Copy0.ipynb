{
 "metadata": {
  "name": "",
  "signature": "sha256:36f0e550c203fa1bd11d3a4565de3a86ebceb57ee227ad6df35067d828064b61"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "u'/Users/Ikkei/data-analysis/GA/class/day8'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "tweets = pd.read_csv('/Users/Ikkei/data-analysis/DAT18NYC/data/twitter5.csv')\n",
      "tweets = tweets\n",
      "\n",
      "tweets.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>tweet</th>\n",
        "      <th>hashtag</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 522205943074160640</td>\n",
        "      <td> Is this #HongKong 's Rodney King? Police need ...</td>\n",
        "      <td> hongkong</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 521669229188501504</td>\n",
        "      <td> 'We won't move and I'm ready to get arrested',...</td>\n",
        "      <td> hongkong</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 522228472786067456</td>\n",
        "      <td> RT @stanyee: Footage of beating prompts #HongK...</td>\n",
        "      <td> hongkong</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 522228386002108418</td>\n",
        "      <td> What is happening in Hong Kong is something th...</td>\n",
        "      <td> hongkong</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 522228373964480512</td>\n",
        "      <td> #Funding:#HongKong #travel #startup @KlookTrav...</td>\n",
        "      <td> hongkong</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "                   id                                              tweet  \\\n",
        "0  522205943074160640  Is this #HongKong 's Rodney King? Police need ...   \n",
        "1  521669229188501504  'We won't move and I'm ready to get arrested',...   \n",
        "2  522228472786067456  RT @stanyee: Footage of beating prompts #HongK...   \n",
        "3  522228386002108418  What is happening in Hong Kong is something th...   \n",
        "4  522228373964480512  #Funding:#HongKong #travel #startup @KlookTrav...   \n",
        "\n",
        "    hashtag  \n",
        "0  hongkong  \n",
        "1  hongkong  \n",
        "2  hongkong  \n",
        "3  hongkong  \n",
        "4  hongkong  "
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tweets.id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "1496"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tweets.id.unique())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "1139"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(tweets.id.isin([522228373964480512]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Finding uniqueness of tweets\n",
      "\n",
      "One important thing to do would be to find the uniqueness of a dataset. Here, we should measure uniqueness as number of unique tweets / number of tweets in the data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tweet_uniqueness(tweets):\n",
      "    # Code for finding the number of unique tweets in a column over the number of tweets.\n",
      "    # should return a number between 0 and 1\n",
      "        \n",
      "    tweets.isin()\n",
      "    \n",
      "# shows that with the code above, we didn't get completely unique tweets.\n",
      "print tweet_uniqueness(tweets.id) \n",
      "\n",
      "# # code to drop duplicates based on the id column alone.\n",
      "unique_tweets = tweets.drop_duplicates(subset=['id'])\n",
      "\n",
      "print len(unique_tweets)\n",
      "# print tweet_uniqueness(unique_tweets.id)\n",
      "# print tweet_uniqueness(unique_tweets.tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "1139\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even by removing the duplicate rows of data, we still have a significant amount of tweets that are the same, likely due to retweets.\n",
      "\n",
      "How could we confirm this hypotheses with pandas? Based on this limited dataset, what hashtag seems to get the most retweeted, vs the least retweeted?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in unique_tweets.hashtag.unique():\n",
      "    print i, tweet_uniqueness(unique_tweets[unique_tweets.hashtag == i].tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hongkong 0\n",
        "occupycentral 0\n",
        "umbrellarevolution 0\n",
        "china 0\n",
        "hk 0\n",
        "admiralty 0\n",
        "occupyhk 0\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = nltk.tokenize.wordpunct_tokenize(tweets.iloc[0,1])\n",
      "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
      "words =  [w for w in words if w not in stopwords]\n",
      "words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "['Is',\n",
        " '#',\n",
        " 'HongKong',\n",
        " \"'\",\n",
        " 'Rodney',\n",
        " 'King',\n",
        " '?',\n",
        " 'Police',\n",
        " 'need',\n",
        " 'answer',\n",
        " 'apparent',\n",
        " 'gratuitous',\n",
        " 'assault',\n",
        " '#',\n",
        " 'OccupyCentral',\n",
        " 'protester',\n",
        " 'https',\n",
        " '://',\n",
        " '.',\n",
        " 'co',\n",
        " '/',\n",
        " 'DGWbo2VCb2']"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words.pop(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "def tokenize_tweet(t, remove_stop=True, remove_hashtag=False):\n",
      "    import string\n",
      "    import re\n",
      "    tweet = t\n",
      "    tweet = tweet.lower()\n",
      "    tweet = re.sub('@\\w+', 'TWITTER_HANDLE', tweet)\n",
      "    tweet = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?', 'URL', tweet)\n",
      "    tweet = tweet.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "    words = nltk.tokenize.wordpunct_tokenize(tweet)\n",
      "    if remove_stop:\n",
      "        # How do we filter for words in the stopwords corpus?\n",
      "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
      "        words = [w for w in words if w not in stopwords]\n",
      "        \n",
      "    if remove_hashtag:\n",
      "        # How do we filter out the actual hashtag in the tweet itself?\n",
      "        pass\n",
      "    \n",
      "    return words\n",
      "\n",
      "\n",
      "unique_tweets['tokens'] = unique_tweets.tweet.apply(tokenize_tweet, remove_stop=True)\n",
      "unique_tweets['tokens_w_stopwords'] = unique_tweets.tweet.apply(tokenize_tweet, remove_stop=False)\n",
      "\n",
      "unique_tweets['tokens_w_stopwords']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "0     [is, this, hongkong, s, rodney, king, police, ...\n",
        "1     [we, wont, move, and, im, ready, to, get, arre...\n",
        "2     [rt, TWITTERHANDLE, footage, of, beating, prom...\n",
        "3     [what, is, happening, in, hong, kong, is, some...\n",
        "4     [fundinghongkong, travel, startup, TWITTERHAND...\n",
        "5     [hk, police, use, pepper, spray, on, protester...\n",
        "6     [rt, TWITTERHANDLE, footage, of, beating, prom...\n",
        "7     [rt, TWITTERHANDLE, footage, of, beating, prom...\n",
        "8     [squeezed, vscocam, vscohub, vscogang, vscogra...\n",
        "9     [rt, TWITTERHANDLE, footage, of, beating, prom...\n",
        "10    [breaking, hongkong, security, chief, says, 6,...\n",
        "11    [trendingnews, hongkong, demonstranten, aangek...\n",
        "12    [TWITTERHANDLE, in, hongkong, umbrellamovement...\n",
        "13    [TWITTERHANDLE, in, hongkong, umbrellamovement...\n",
        "14    [map, of, the, underpass, in, hongkong, where,...\n",
        "...\n",
        "1476    [rt, TWITTERHANDLE, five, hk, press, unions, u...\n",
        "1477    [rt, TWITTERHANDLE, 911pm, rodney, street, pac...\n",
        "1478    [rt, TWITTERHANDLE, packed, around, the, stage...\n",
        "1479              [rt, TWITTERHANDLE, now, occupyhk, URL]\n",
        "1481    [rt, TWITTERHANDLE, this, is, the, post, start...\n",
        "1482    [rt, TWITTERHANDLE, urgently, need, face, mask...\n",
        "1484    [rt, TWITTERHANDLE, occupied, area, stretches,...\n",
        "1485    [police, cant, stand, the, verbal, abuse, from...\n",
        "1486    [rt, TWITTERHANDLE, barricades, also, building...\n",
        "1487    [album, of, close, up, shots, of, the, pepper,...\n",
        "1488    [rt, TWITTERHANDLE, calling, wooden, plates, h...\n",
        "1490    [rt, TWITTERHANDLE, lung, wo, rd, what, source...\n",
        "1491    [rt, TWITTERHANDLE, big, pictures, from, TWITT...\n",
        "1492    [rt, TWITTERHANDLE, now, amp, signing, off, oc...\n",
        "1493    [rt, TWITTERHANDLE, this, amount, of, people, ...\n",
        "Name: tokens_w_stopwords, Length: 1139, dtype: object"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the following question to answer:\n",
      "\n",
      "Do different audiences use different hashtags surrounding the hong kong protests?\n",
      "\n",
      "We can use nltk to help break apart the words in the tweets and determine how words may change based on the hashtag.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pos_tag is a part of speech tagger, based on the text that it ingests. \n",
      "# while its not built for twitter data, we can try it out and see how accurate it is\n",
      "nltk.pos_tag\n",
      "unique_tweets['pos'] = unique_tweets['tokens'].apply(nltk.pos_tag)\n",
      "\n",
      "# Printing out all words that come back as adjectives (JJ):\n",
      "adjectives = []\n",
      "for i in unique_tweets.pos:\n",
      "    bb = [j[0] for j in i if j[1] == 'JJ']\n",
      "    if bb:\n",
      "        adjectives.extend(bb)\n",
      "\n",
      "print list(set(adjectives))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['heutejournal', 'malicious', 'excessive', 'global', 'yellow', 'kong', 'top', 'govt', 'violent', 'asian', 'human', 'japan', 'taiwan', 'hate', 'blow', '116th', 'le', 'recipient', 'young', 'easy', 'rabble', 'cardinal', 'govern', 'te', 'real', 'good', 'animal', 'big', 'stop', 'possible', 'early', 'joint', 'traffic', 'shameful', 'anonymous', 'international', 'front', 'trouble', 'occupyhongkong', 'helpful', 'cable', 'tear', 'large', 'investigate', 'small', 'insane', 'financial', 'verbal', 'national', 'dead', 'hair', 'likely', 'economic', '3nsailing', 'agricultural', 'corrupt', 'sexual', 'special', 'viral', 'tungchung', 'symbiotic', 'bad', 'legal', 'creative', 'current', 'outside', 'various', 'new', 'adorable', 'public', '3d', 'available', 'suisheng', 'full', 'christian', 'pathetic', 'free', 'strong', 'soundcloud', 'jubilant', 'sheung', 'prodemocratic', 'great', 'bible', 'central', 'standby', 'many', 'aguus', 'equal', 'urged', 'foreign', 'protestorsoccupycentral', 'social', 'military', 'weird', 'create', 'first', 'major', 'industrial', 'vid', 'civic', 'irresponsible', 'powerful', 'arsenal', 'private', 'supporthongkong', 'lung', 'female', 'spanish', 'open', 'legislative', 'wan', 'additive', 'silent', 'ophongkong', 'angry', 'fiveyear', 'genetic', 'next', 'occupy', 'live', 'occupycentral', 'low', 'peaceful', 'australian', '3dprinting', 'website', 'british', 'gear', 'riot', 'successful', 'basic', 'alive', 'icable', 'last', 'general', 'former', 'main', 'dangerous', 'outrageous', 'sous', 'future', 'obvious', 'dirty', 'ive', 'gratuitous', 'chaotic', 'pic', 'site', 'overzealous', 'comfortable', 'high', 'middle', 'brutal', 'want', 'ready', 'wonderful', 'david', 'occupied', 'eric', 'physical', 'opportunityfinancial', 'kush', 'hong', 'hongkong', 'mic', 'responsible', 'hot', 'republic', 'fourth', 'automotive', 'widespread', 'several', 'beach', 'poor', 'independent', 'finish', 'normal', 'impartial', 'english', 'evil', 'sigue', 'wrong', 'serious', 'recent', 'natural', 'third', 'clear', 'proud', 'acknowledged', 'thought', 'concrete', 'tech', 'african', 'professional', 'democratic', 'microsoft', 'unspeakable']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What are some of the challenges that this tagger seems to face?\n",
      "\n",
      "Above it's clear it does... okay. Since pos_tag trains based on its input, we could theoretically from here create a corpus that makes up for the expected English patterns, and then it will be better calibrated. For today, let's consider a smaller list of adjectives based on what nltk believes it found for adjectives.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adjective_list = {\n",
      "    'industrial': 0,\n",
      "    'excessive': -1,\n",
      "    'gratuitous': 1,\n",
      "    'chaotic': -1,\n",
      "    'national': 1,\n",
      "    'young': 1,\n",
      "    'yellow': 0,\n",
      "    'high': 1,\n",
      "    'middle': 0,\n",
      "    'likely': 1,\n",
      "    'economic': 0,\n",
      "    'creative': 1,\n",
      "    'open': 1,\n",
      "    'physical': 0,\n",
      "    'symbiotic': 1,\n",
      "    'legal': 1,\n",
      "    'next': 1,\n",
      "    'genetic': 0,\n",
      "    'angry': -1,\n",
      "    'strong': 1,\n",
      "    'peaceful': 1,\n",
      "    'new': 1,\n",
      "    'widespread': 1,\n",
      "    'real': 1,\n",
      "    'good': 1,\n",
      "    'normal': 0,\n",
      "    'successful': 1,\n",
      "    'big': 1,\n",
      "    'basic': -1,\n",
      "    'hate': -1,\n",
      "    'private': -1,\n",
      "    'front': 0,\n",
      "    'central': 0,\n",
      "    'comfortable': 1,\n",
      "    'last': 0,\n",
      "    'helpful': 1,\n",
      "    'third': 0,\n",
      "    'many': 1,\n",
      "    'clear': 1,\n",
      "    'proud': 1,\n",
      "    'brutal': -1,\n",
      "    'large': 1,\n",
      "    'dirty': -1,\n",
      "    'professional': 1,\n",
      "    'first': 0,\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Goal: Using the adjectives and some measure of sentiment, predict a given hashtag?**\n",
      "\n",
      "We'll need to build a couple more functions:\n",
      "\n",
      "1. Let's write a function that creates a sentiment score for each tweet based on the adjectives above.\n",
      "2. Set the targets of the model. We can do this two ways:\n",
      "    1. One, using the `hashtag` column, which is what was generated based on the twitter search\n",
      "    2. Create several target columns, based on the tweet itself, using a regex match for the hashtag.\n",
      "3. Finally, build a logistic regression using NLTK's sklearn implementation using our created sentiment as a regressor."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First function: create a sentiment score column.\n",
      "# Should take in a list of words, and return back the score as\n",
      "# mean(sentiment_of_adjectives)\n",
      "def measure_sentiment(words):\n",
      "\n",
      "\n",
      "# Second function: Create a numeric target column.\n",
      "def numeric_hashtag(tag):\n",
      "    # we could use a dictionary similar to above to easily map hashtags to numeric values.\n",
      "    targets = {\n",
      "        u'hongkong': 0,\n",
      "        u'occupycentral': 1,\n",
      "        u'umbrellarevolution':2,\n",
      "        u'china':3,\n",
      "        u'hk':4,\n",
      "        u'admiralty':5,\n",
      "        u'occupyhk':6,\n",
      "    }\n",
      "    return targets[tag]\n",
      "\n",
      "unique_tweets['sentiment'] = unique_tweets.tokens.apply(measure_sentiment)\n",
      "print unique_tweets.sentiment.hist()\n",
      "\n",
      "unique_tweets['target'] = unique_tweets.hashtag.apply(numeric_hashtag)\n",
      "\n",
      "# Build a logistic regression using the sentiment feature and the numeric hashtags\n",
      "from sklearn import linear_model as lm\n",
      "\n",
      "lmfit = lm.LogisticRegression()\n",
      "lmfit.fit(unique_tweets[['sentiment']], unique_tweets['target'])\n",
      "print lmfit.score(unique_tweets[['sentiment']], unique_tweets['target'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Axes(0.125,0.125;0.775x0.775)\n",
        "0.197541703248"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNn9JmkI2f0maQv8NiaD3qfuWJKoAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x112e35650>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Some things we didn't talk about\n",
      "\n",
      "A common approach to handling text data is working with term frequencies and inverse document frequencies. sk-learn actually has a TF-IDF vectorizer that would be useful to interact with. A TF vectorizer would also be a fine start when working with larger texts.\n",
      "\n",
      "These matrices (TF and TF-IDF) are also strong indicators for classification.\n",
      "Consider the usage of stopwords. Stopswords can actually be incredibly useful in predicting! (http://rforwork.info/2012/12/27/intro-to-mult-classification/)\n",
      "\n",
      "## Next steps/Homework\n",
      "Continue iterating on the twitter data set. What new features could you curate that could be predictive of a hashtag?\n",
      "\n",
      "Consider:\n",
      "\n",
      "1. length of a tweet, or number of tokens in a tweet\n",
      "2. Finding more words to include in the sentiment dictionary\n",
      "3. Finding words most commonly used along a hashtag.\n",
      "\n",
      "add to the code, and build a better model using your new features.\n",
      "\n",
      "**OR**\n",
      "\n",
      "follow a different persuit with the data, given that you can work with the twitter api, which will provide you with much more data than included in the master csv file here.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}