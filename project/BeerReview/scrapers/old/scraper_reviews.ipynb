{
 "metadata": {
  "name": "",
  "signature": "sha256:8ece6e4ea17290ba8a6c4a39f2a8d8726a1b2730e040d70cee3cdf1898a4a37b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "from multiprocessing.pool import ThreadPool\n",
      "import urllib2\n",
      "import bs4\n",
      "import pandas as pd\n",
      "import pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ratebeer_url = r'http://www.ratebeer.com'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_soup(url):\n",
      "    \"\"\"\n",
      "    Get BeautifulSoup's soup object from an url\n",
      "    \"\"\"\n",
      "    request = urllib2.Request(url)\n",
      "    response = urllib2.urlopen(request)\n",
      "    return bs4.BeautifulSoup(response)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_basic_beer_info(soup):\n",
      "    \"\"\"\n",
      "    Scrape basic beer info part, such as name, description, or ratings\n",
      "    \"\"\"\n",
      "    beer_info = {}\n",
      "    \n",
      "    beer_info['name'] = soup.select('div.user-header')[0].get_text()\n",
      "\n",
      "    for image in soup.select('div a[href^=http://res.cloudinary.com/ratebeer/image/upload/]'):    \n",
      "        beer_info['beer_img'] = image['href']\n",
      "        beer_info['beer_img_src'] = image.find('img')['src']\n",
      "\n",
      "    try:\n",
      "        beer_info['beerstyles'] = soup.select('a[href^=/beerstyles/]')[0]['href']\n",
      "    except IndexError:\n",
      "        beer_info['beerstyles'] = ''\n",
      "    \n",
      "    try:\n",
      "        beer_info['bottle_type'] = soup.find('a', {'rel': 'modal:open'}).get_text()\n",
      "    except AttributeError:\n",
      "        beer_info['bottle_type'] = ''\n",
      "    try:\n",
      "        ratings_str = soup.find('div', {'style': 'padding: 0px 10px; font-weight: bold; font-size: 12px;'}).get_text()\n",
      "        rating_tmp = re.split(u'\\xa0\\xa0|:', ratings_str)\n",
      "\n",
      "        ratings = {}\n",
      "        for i in range(len(rating_tmp)):\n",
      "            if i % 2 == 0:\n",
      "                fld = re.sub(r' |\\. ', '_', rating_tmp[i].strip()).lower()\n",
      "                val = rating_tmp[i + 1].lower().strip('%')\n",
      "                ratings[fld] = val        \n",
      "        beer_info['ratings'] = ratings\n",
      "    except AttributeError:\n",
      "        beer_info['ratings'] = ''\n",
      "    \n",
      "    try:\n",
      "        des = soup.find('div', {'style': 'border: 1px solid #e0e0e0; background: #fff; padding: 14px; color: #777;'})\n",
      "        beer_info['des'] = des.getText().strip('COMMERCIAL DESCRIPTION')\n",
      "    except AttributeError:\n",
      "        beer_info['des'] = ''\n",
      "        \n",
      "    beer_info['max_page'] = 1\n",
      "    for page in soup.select('a.ballno'):\n",
      "        page_tmp = int(page.getText())\n",
      "        if page_tmp > beer_info['max_page']:\n",
      "            beer_info['max_page'] = page_tmp\n",
      "    \n",
      "    return beer_info"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chunks(lst, chunk_size):\n",
      "    \"\"\" Yield successive n-sized chunks from l.\n",
      "    \"\"\"\n",
      "    for i in xrange(0, len(lst), chunk_size):\n",
      "        yield lst[i:i + chunk_size]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_reviews(soup):\n",
      "    \"\"\"\n",
      "    Scrape beer review part\n",
      "    \"\"\"\n",
      "    user_ids = []\n",
      "    review_dates = []\n",
      "    bold_scores = []\n",
      "    detail_scores = []\n",
      "    review_txts = []\n",
      "    score_names = ['aroma', 'apearance', 'taste', 'palate', 'overall']\n",
      "    reviews = {}\n",
      "    \n",
      "    # Get all the fields separately\n",
      "    for table in soup.findAll('table', attrs={'style':'padding: 10px;'}):\n",
      "        for small in table.findAll('small', {'style': 'color: #666666; font-size: 12px; font-weight: bold;'}):\n",
      "            try:\n",
      "                date_str = small.get_text()\n",
      "                date_str = date_str.split('-')[2].strip()\n",
      "                review_dates.append(date_str)\n",
      "            except:\n",
      "                review_dates.append('')\n",
      "        \n",
      "        for user_id in soup.select('a[href^=/user/]'):\n",
      "            id_tmp = int(user_id['href'].split('/')[2])\n",
      "            user_ids.append(id_tmp)\n",
      "\n",
      "        for item in table.findAll('div', {'style': 'display:inline; padding: 0px 0px; font-size: 24px; font-weight: bold; color: #036;'}):\n",
      "            bold_scores.append(item.get_text())\n",
      "\n",
      "        # AROMA, APPEARANCE, TASTE, PALATE, OVERALL\n",
      "        for item in table.findAll('big', {'style': 'color: #999;'}):\n",
      "            detail_scores.append(item.getText().split(r'/')[0])                                    \n",
      "\n",
      "        for review in table.findAll('div', {'style': 'padding: 20px 10px 20px 0px; border-bottom: 1px solid #e0e0e0; line-height: 1.5;'}):\n",
      "            review_txts.append(review.get_text().strip())\n",
      "    \n",
      "    # Combine fields by reviews\n",
      "    detail_scores_chunk = chunks(detail_scores, 5)\n",
      "    for idx in range(len(user_ids)):\n",
      "        review_tmp = {}\n",
      "        review_tmp['bold_score'] = bold_scores[idx]\n",
      "        review_tmp['review'] = review_txts[idx]\n",
      "        review_tmp['review_date'] = review_dates[idx]\n",
      "        \n",
      "        scores_tmp = {score_names[i]:score for i, score in enumerate(detail_scores_chunk.next())}\n",
      "        \n",
      "        reviews[user_ids[idx]] = dict(review_tmp.items() + scores_tmp.items())\n",
      "    \n",
      "    return reviews"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get a list of beers that was scraped before\n",
      "f = open('beers.txt', 'r')\n",
      "beers = [line.strip().split('\\t') for line in f.readlines()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beers[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[['13357', '192012', '/beer/mystery-white-tiger/192012/'],\n",
        " ['13357', '270656', '/beer/mystery-wisping-rune/270656/'],\n",
        " ['13357', '179317', '/beer/mystery-adamanthea/179317/'],\n",
        " ['13357', '301630', '/beer/mystery-admiration/301630/'],\n",
        " ['13357', '309313', '/beer/mystery-alexandre/309313/']]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape(beer):\n",
      "    \"\"\"\n",
      "    This is the main scraper function that connects every helper functions\n",
      "    \"\"\"\n",
      "    beer_id = beer[1]\n",
      "    beer_url = beer[2]\n",
      "    \n",
      "    beer_data = {}\n",
      "    \n",
      "    try:\n",
      "        # Scrape once for all the beers\n",
      "        soup = get_soup(root_url + beer_url + '/1/1/')\n",
      "\n",
      "        # Beer basic info\n",
      "        info_tmp = get_basic_beer_info(soup)\n",
      "        beer_data[beer_id] = info_tmp\n",
      "        # Beer reviews\n",
      "        beer_data['reviews'] = get_reviews(soup)\n",
      "\n",
      "        # Scrape again if there are more than one pages\n",
      "        if info_tmp['max_page'] > 1:\n",
      "            for page_num in xrange(1, info_tmp['max_page']):\n",
      "                page_num += 1\n",
      "\n",
      "                soup = get_soup(root_url + beer_url + '/1/%s/' % (page_num))\n",
      "                review_tmp = get_reviews(soup)           \n",
      "                beer_data['reviews'] = dict(beer_data['reviews'].items() + review_tmp.items())\n",
      "    except Exception, e:\n",
      "        print beer_url\n",
      "        print e\n",
      "        print\n",
      "        \n",
      "#     return beer_info, beer_reviews\n",
      "    return beer_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pool = ThreadPool(8)\n",
      "for idx, beer_cunk in enumerate(chunks(beers, 10000)):\n",
      "    results = (pool.map(scrape, beer_cunk))\n",
      "    \n",
      "    filename = '/home/ikkei/Documents/GA/project/BeerReview/data/reviews/review%s.pickle' % (idx)\n",
      "    with open(filename, 'wb') as f:\n",
      "        pickle.dump(results, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}